{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a0eab7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a class for dataset loading\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "import cv2\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df17baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_dir = \"../dataset/crop_224/\"\n",
    "labels_dir = \"../dataset/AppleV_intrinsic.csv\"\n",
    "\n",
    "class AppleDataset(Dataset):\n",
    "    def __init__(self, images_dir, labels_dir):\n",
    "        self.labels = pd.read_csv(labels_dir)\n",
    "        self.images = os.listdir(images_dir)\n",
    "\n",
    "        # load the images and labels in a list\n",
    "        self.img_data = []\n",
    "        self.img_labels = []\n",
    "\n",
    "        for image in self.images:\n",
    "            img_path = os.path.join(images_dir, image)\n",
    "            img = cv2.imread(img_path)\n",
    "            img = np.float32(img) / 255.0\n",
    "            self.img_data.append(img)\n",
    "\n",
    "            image = image.split(\"_\")[0]+'.jpg'\n",
    "            t = self.labels[self.labels['filenames'] == image][['TSS_brix', 'Firmness', 'Titerabile acid percentage']].values\n",
    "            self.img_labels.append(t)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def arguement(self, img, rotTimes, vFlip, hFlip):\n",
    "        # Random rotation\n",
    "        for j in range(rotTimes):\n",
    "            img = np.rot90(img.copy(), axes=(1, 2))\n",
    "        # Random vertical Flip\n",
    "        for j in range(vFlip):\n",
    "            img = img[:, :, ::-1].copy()\n",
    "        # Random horizontal Flip\n",
    "        for j in range(hFlip):\n",
    "            img = img[:, ::-1, :].copy()\n",
    "        return img\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.img_data[idx]\n",
    "        label = self.img_labels[idx]\n",
    "\n",
    "        rotTimes = random.randint(0, 3)\n",
    "        vFlip = random.randint(0, 1)\n",
    "        hFlip = random.randint(0, 1)\n",
    "        image = self.arguement(image, rotTimes, vFlip, hFlip)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ca28b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "class VGG19(nn.Module):\n",
    "    def __init__(self, input_size=(3, 224, 224), output_size=3, finetune=False):\n",
    "        super(VGG19, self).__init__()\n",
    "        \n",
    "        # Load pre-trained VGG19 model\n",
    "        model = models.vgg19(weights='IMAGENET1K_V1')\n",
    "\n",
    "        # Freeze convolutional layers\n",
    "        for param in model.features.parameters():\n",
    "            param.requires_grad = not finetune\n",
    "\n",
    "        # Modify the classifier: to reduce the number of params \n",
    "        model.classifier[0] = nn.Linear(25088, 2048)  # Output layer with the desired output size\n",
    "        model.classifier[3] = nn.Linear(2048, 2048)\n",
    "        model.classifier[6] = nn.Linear(2048, output_size)\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c65b4320",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Function to calculate metrics\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    y_true = y_true.detach().cpu().numpy()\n",
    "    y_pred = y_pred.detach().cpu().numpy()\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return mse, mae, r2\n",
    "\n",
    "def write_to_csv(model_name, num_epochs, train_losses, val_losses, train_metrics, val_metrics, total_params, trainable_params, non_trainable_params):\n",
    "    # Save results to CSV\n",
    "    results = {\n",
    "        \"Epoch\": list(range(1, num_epochs + 1)),\n",
    "        \"Train Loss\": train_losses,\n",
    "        \"Train MSE\": train_metrics[\"MSE\"],\n",
    "        \"Train MAE\": train_metrics[\"MAE\"],\n",
    "        \"Train R2\": train_metrics[\"R2\"],\n",
    "    }\n",
    "    \n",
    "    # Add parameter counts as additional rows (repeated across all rows for clarity)\n",
    "    results[\"Total Params\"] = [total_params] * num_epochs\n",
    "    results[\"Trainable Params\"] = [trainable_params] * num_epochs\n",
    "    results[\"Non-trainable Params\"] = [non_trainable_params] * num_epochs\n",
    "\n",
    "    results[\"Val Loss\"] = val_losses * num_epochs\n",
    "    results[\"Val MSE\"] = val_metrics[\"MSE\"] * num_epochs\n",
    "    results[\"Val MAE\"] = val_metrics[\"MAE\"] * num_epochs\n",
    "    results[\"Val R2\"] = val_metrics[\"R2\"] * num_epochs\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(f\"results/{model_name}.csv\", index=False)\n",
    "    print(\"Results saved to 'training_results_with_params.csv'\")\n",
    "\n",
    "def calculate_model_parameters(model):\n",
    "    # Count model parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    non_trainable_params = total_params - trainable_params\n",
    "\n",
    "    print(f\"Total parameters: {total_params}\")\n",
    "    print(f\"Trainable parameters: {trainable_params}\")\n",
    "    print(f\"Non-trainable parameters: {non_trainable_params}\")\n",
    "    return total_params, trainable_params, non_trainable_params\n",
    "\n",
    "\n",
    "def train_model(model_name, model, train_loader, val_loader, criterion, optimizer, num_epochs=10, device='cuda'):\n",
    "    model = model.to(device)\n",
    "    total_params, trainable_params, non_trainable_params = calculate_model_parameters(model)\n",
    "\n",
    "    # Early stopping parameters\n",
    "    patience = 5\n",
    "    best_train_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    # Lists to store metrics\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_metrics = {\"MSE\": [], \"MAE\": [], \"R2\": []}\n",
    "    val_metrics = {\"MSE\": [], \"MAE\": [], \"R2\": []}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        y_true_train, y_pred_train = [], []\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate loss and predictions\n",
    "            running_loss += loss.item()\n",
    "            y_true_train.append(labels)\n",
    "            y_pred_train.append(outputs)\n",
    "            break\n",
    "\n",
    "        # Normalize training loss\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Calculate training metrics\n",
    "        y_true_train = torch.cat(y_true_train, dim=0)\n",
    "        y_pred_train = torch.cat(y_pred_train, dim=0)\n",
    "        mse, mae, r2 = calculate_metrics(y_true_train, y_pred_train)\n",
    "        train_metrics[\"MSE\"].append(mse)\n",
    "        train_metrics[\"MAE\"].append(mae)\n",
    "        train_metrics[\"R2\"].append(r2)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {train_loss:.4f}, MSE: {mse:.4f}, MAE: {mae:.4f}, R2: {r2:.4f}\")\n",
    "\n",
    "        # Early stopping logic\n",
    "        if train_loss < best_train_loss:\n",
    "            best_train_loss = train_loss\n",
    "            epochs_no_improve = 0\n",
    "            # Optionally save the best model\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve == patience:\n",
    "            print(f\"Early stopping triggered. No improvement for {patience} epochs.\")\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    y_true_val, y_pred_val = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Accumulate validation loss and predictions\n",
    "            val_loss += loss.item()\n",
    "            y_true_val.append(labels)\n",
    "            y_pred_val.append(outputs)\n",
    "            break\n",
    "\n",
    "    # Normalize validation loss\n",
    "    val_loss /= len(val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    # Calculate validation metrics\n",
    "    y_true_val = torch.cat(y_true_val, dim=0)\n",
    "    y_pred_val = torch.cat(y_pred_val, dim=0)\n",
    "    mse, mae, r2 = calculate_metrics(y_true_val, y_pred_val)\n",
    "    val_metrics[\"MSE\"].append(mse)\n",
    "    val_metrics[\"MAE\"].append(mae)\n",
    "    val_metrics[\"R2\"].append(r2)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Validation Loss: {val_loss:.4f}, MSE: {mse:.4f}, MAE: {mae:.4f}, R2: {r2:.4f}\")\n",
    "\n",
    "    ## Save the model's state dictionary\n",
    "    # torch.save(model.state_dict(), f\"saved_models/{model_name}.pth\")\n",
    "    # print(f'Model saved at saved_models/{model_name}.pth')\n",
    "\n",
    "    # write_to_csv(model_name, num_epochs, train_losses, val_losses, train_metrics, val_metrics, total_params, trainable_params, non_trainable_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0aa62e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "93ca9100",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AppleDataset(images_dir, labels_dir)\n",
    "val_size = int(0.2 * len(train_dataset))\n",
    "train_size = len(train_dataset) - val_size\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2ff73ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 75609155\n",
      "Trainable parameters: 75609155\n",
      "Non-trainable parameters: 0\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model = VGG19(input_size=(3, 224, 224), output_size=3, finetune=False)\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()  # For regression (use BCEWithLogitsLoss for multi-label classification)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Assuming you have a Da taLoader ready as `data_loader`\n",
    "device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "num_epochs = 1\n",
    "train_model(model, model, train_loader, val_loader, criterion, optimizer, num_epochs=num_epochs, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a65439",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytoch_venv",
   "language": "python",
   "name": "pytoch_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
